{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa7da3c-9941-44a0-b9a6-1d0e440ac903",
   "metadata": {},
   "source": [
    "# 再探CNN卷积核\n",
    "\n",
    "如果学过信号系统或者傅里叶变换等基础数学的话，一定听说过卷积运算，基本概念可以参考[如何通俗易懂地解释卷积？](https://www.zhihu.com/question/22298352)。\n",
    "\n",
    "简而言之，卷积运算可以理解为：有两个序列，其中一个翻转后再和另一个平移做点积，得到一个新的序列的过程。\n",
    "\n",
    "那么CNN卷积核执行的运算是不是这样的呢？\n",
    "\n",
    "首先，看一看原始的卷积运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fda7806-681d-40ac-9228-3eef8dfed16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7467a1e9-82a3-41cb-a986-0e2bfeca59d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 1.9, 3.7, 1.9, 1. ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.array([0.3, 0.2, 0.5])\n",
    "a2 = np.array([5, 3, 2])\n",
    "np.convolve(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c391b126-5244-4846-9397-afc2a5715e85",
   "metadata": {},
   "source": [
    "PyTorch中没有直接的convovle运算。有的是神经网络中的卷积运算。\n",
    "\n",
    "functional中有[conv1d函数](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d)，\n",
    "\n",
    "先简单补充一点CNN中的卷积计算输出尺寸计算的公式（参考[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d)）。\n",
    "\n",
    "$$N=\\lfloor{\\frac{W-F+2P}{S}+1}\\rfloor$$\n",
    "\n",
    "其中，N为输出尺寸，W是输入的，F是卷积核大小，P是padding填充的大小，S是步长stride。\n",
    "\n",
    "比如输入大小是3，F卷积核也是3，没有P，S为1，那么输出大小就为1；如果我们想在输入和卷积核大小不变的情况下，加大输出尺寸，那么需要padding。令P=2，可以看到输出就是5了。\n",
    "\n",
    "对于二维是一样的，只不过宽和高各一个计算公式。\n",
    "\n",
    "尺寸是针对一个feature（即channel）的“图像”大小的，对于一个[batch, feature, sequence]的张量，一维卷积是合适的；如果输入输出的feature都是1，那么卷积核就是 1\\*1\\*kernel_width  ($\\text{out_channels} , \\text{in_channels} , kW)$的大小了。\n",
    "\n",
    "也就是说，一维卷积输入张量各维度分别是 batch-channel_in-width。卷积核是参数weight（bias默认为None），各维度为 channel-channel_in-width （更多内容参考[文档](https://pytorch.org/docs/stable/generated/torch.nn.functional.conv1d.html#torch.nn.functional.conv1d)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8132ad7-3503-4897-8903-99750a11ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a14f8e2-8d18-4420-9489-159c171f6775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.1000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(np.array([[[5, 3, 2]]]))\n",
    "filters = torch.Tensor(np.array([[[0.3, 0.2, 0.5]]]))\n",
    "F.conv1d(inputs, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0943745-9a6e-4124-b35e-0f6d5f7afe8c",
   "metadata": {},
   "source": [
    "可以看到，卷积计算就是直接进行的乘积求和。下面我们手动加上padding和翻转，看看结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0234bc5f-6522-4615-8807-09264cd14cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5000, 1.9000, 3.7000, 1.9000, 1.0000]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(np.array([[[0, 0, 5, 3, 2, 0, 0]]]))\n",
    "filters = torch.Tensor(np.array([[[0.5, 0.2, 0.3]]]))\n",
    "F.conv1d(inputs, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c5e42-3c2f-4a32-a4f7-d3e56fd9795a",
   "metadata": {},
   "source": [
    "可以看到，加入padding和翻转filter之后可以达到同样的效果，所以这也是为什么虽然没有完全和convolve的定义一样但是仍然称作卷积的原因。本质上仍是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbd0e67d-c4fc-4f41-ad8a-3f5525011c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.5000, 1.9000, 3.7000, 1.9000, 1.0000]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(np.array([[[5, 3, 2]]]))\n",
    "filters = torch.Tensor(np.array([[[0.5, 0.2, 0.3]]]))\n",
    "F.conv1d(inputs, filters, padding=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c594f37-8647-4a89-b28f-f51c84e374e0",
   "metadata": {},
   "source": [
    "在水文计算中，也经常遇到卷积运算，比如单位线，瞬时单位线可以看作是单位冲激函数在线性时变系统上的响应。不过也有的模型虽然采用单位线的说法，但是更倾向于指明时段产流在后续给时段上的分配情况，比如GR4J模型中的unit hydrograph。本质上来说，两者应该是一致的，都是强调单位时段水量在后续各时段上的重新分布。这里我们以后一种为例，这时候各时段坐标值之和为1（水量守恒）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2a96014-9a1b-4e25-bd7a-8b9ffbd598f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_curves1(t, x4):\n",
    "    \"\"\"\n",
    "        Unit hydrograph ordinates for UH1 derived from S-curves.\n",
    "    \"\"\"\n",
    "\n",
    "    if t <= 0:\n",
    "        return 0\n",
    "    elif t < x4:\n",
    "        return (t / x4) ** 2.5\n",
    "    else:  # t >= x4\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "229e23da-f145-445d-9d06-583b7df09035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06415002990995841, 0.29873733939125313, 0.6371126306987884]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "X4=3\n",
    "nUH1 = int(math.ceil(X4))\n",
    "uh1_ordinates = [0] * nUH1\n",
    "for t in range(1, nUH1 + 1):\n",
    "    uh1_ordinates[t - 1] = s_curves1(t, X4) - s_curves1(t - 1, X4)\n",
    "    \n",
    "uh1_ordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104a9d05-f3ea-45e7-9787-8bfdb397e2dc",
   "metadata": {},
   "source": [
    "如果我们想要让卷积核保持和为1的约束，我们可以考虑使用softmax函数，将输入softmax的参数作为优化的对象，生成的变量作为卷积核，可以继续执行卷积运算。\n",
    "\n",
    "先简单回顾下softmax函数，默认的softmax是对每列数据进行运算的，保证每列的数据之和为1，可以通过指定dim来对行运算，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fb1c97a-366a-4d8a-a467-51b7de2f2e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1509, 0.3886, 0.4605],\n",
       "        [0.6525, 0.1027, 0.2448]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "output = m(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb643a6f-27e8-46f9-a862-445632f8b706",
   "metadata": {},
   "source": [
    "functional的softmax函数可以达到一样的效果。看一个三维的tensor的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b7faf02-09c0-470c-a093-fb8d93a295cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3510, 0.2587, 0.3903]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_3d = torch.randn(1,1,3)\n",
    "F.softmax(input_3d, dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc442a9-1399-49b1-8ab5-e0e02740ae22",
   "metadata": {},
   "source": [
    "下面是简单的指定卷积核形式之后的卷积运算（实际应用还需要注意权重参数初始化等），更多实践方法可以参考[这里](https://www.programmersought.com/article/10583895621/)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1c1cebe-4420-4dac-b97f-096eaf03841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KernelConv, self).__init__()        \n",
    "\n",
    "    def forward(self, input, kernel):\n",
    "        # the dim of input: batch-channel_in-width\n",
    "        # the dim of kernel: channel-channel_in/groups-width\n",
    "        uh = F.softmax(kernel, dim=2)\n",
    "        output = F.conv1d(input, uh,padding=kernel.shape[-1]-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c1c7ab0-348f-4e1e-9ccd-85577468f46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.3341, 3.1870, 2.1649, 0.2822, 0.0318]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kConv = KernelConv()\n",
    "inputs = torch.Tensor(np.array([[[5, 3, 2]]]))\n",
    "# 随便给定一组数据作为filter的前序输入\n",
    "filters = torch.Tensor(np.array([[[1, 3, 5]]]))\n",
    "kConv(inputs, filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ea587-9baa-4895-a792-26a7689b30b1",
   "metadata": {},
   "source": [
    "知道以上内容可以帮助我们更进一步地了解汇流计算，并对汇流计算如何与神经网络联系起来有更多的直观认识。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
